{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "219be415-a357-469e-a856-f35db61f513b",
      "metadata": {
        "id": "219be415-a357-469e-a856-f35db61f513b"
      },
      "outputs": [],
      "source": [
        "\"\"\"安装依赖\n",
        "\"\"\"\n",
        "!pip install accelerate diffusers controlnet_aux omegaconf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28e143d2-140e-4a54-bf50-ded84a42bedb",
      "metadata": {
        "id": "28e143d2-140e-4a54-bf50-ded84a42bedb"
      },
      "outputs": [],
      "source": [
        "\"\"\"下载模型\n",
        "\"\"\"\n",
        "!mkdir -p /content/models\n",
        "# stable diffusion 模型\n",
        "!git clone --depth=1 https://huggingface.co/runwayml/stable-diffusion-v1-5 /content/models/stable-diffusion-v1-5\n",
        "# LCM lora 模型\n",
        "!git clone --depth=1 https://huggingface.co/latent-consistency/lcm-lora-sdv1-5 /content/models/lcm-lora-sdv1-5\n",
        "# ControlNet 模型\n",
        "!git clone --depth=1 https://huggingface.co/lllyasviel/ControlNet-v1-1 /content/models/ControlNet-v1-1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02ca1f5b-acff-440e-8e35-307eac822372",
      "metadata": {
        "id": "02ca1f5b-acff-440e-8e35-307eac822372"
      },
      "outputs": [],
      "source": [
        "\"\"\"文生图\n",
        "\"\"\"\n",
        "import torch\n",
        "from IPython.display import display\n",
        "from diffusers import StableDiffusionPipeline, UniPCMultistepScheduler\n",
        "\n",
        "\n",
        "model_id = \"/content/models/stable-diffusion-v1-5\"\n",
        "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
        "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
        "pipe = pipe.to(\"cuda\")\n",
        "\n",
        "#prompt = \"a photo of an astronaut riding a horse on mars\"\n",
        "prompt = [\"(taylor swift), 1girl, best quality, extremely detailed\"]\n",
        "negative_prompt=[\"monochrome, lowres, bad anatomy, worst quality, low quality\"]\n",
        "\n",
        "images = pipe(prompt, negative_prompt=negative_prompt, num_inference_steps=20).images\n",
        "\n",
        "for image in images:\n",
        "    display(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f95c7b55-fbec-4f15-8680-4c95dd081561",
      "metadata": {
        "id": "f95c7b55-fbec-4f15-8680-4c95dd081561"
      },
      "outputs": [],
      "source": [
        "\"\"\"图生图\n",
        "\"\"\"\n",
        "from io import BytesIO\n",
        "\n",
        "import torch\n",
        "import requests\n",
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "from diffusers import StableDiffusionImg2ImgPipeline, UniPCMultistepScheduler\n",
        "\n",
        "model_id = \"/content/models/stable-diffusion-v1-5\"\n",
        "pipe = StableDiffusionImg2ImgPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
        "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
        "pipe = pipe.to(\"cuda\")\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/comfyanonymous/ComfyUI/master/input/example.png\"\n",
        "response = requests.get(url)\n",
        "init_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "init_image = init_image.resize((512, 512))\n",
        "\n",
        "prompt = [\"(taylor swift), 1girl, best quality, extremely detailed\"]\n",
        "negative_prompt=[\"monochrome, lowres, bad anatomy, worst quality, low quality\"]\n",
        "\n",
        "images = pipe(\n",
        "    prompt, negative_prompt=negative_prompt, image=init_image,\n",
        "    strength=0.75, guidance_scale=7.5, num_inference_steps=20\n",
        ").images\n",
        "\n",
        "\n",
        "def image_grid(imgs, rows, cols):\n",
        "    assert len(imgs) == rows * cols\n",
        "\n",
        "    w, h = imgs[0].size\n",
        "    grid = Image.new(\"RGB\", size=(cols * w, rows * h))\n",
        "    grid_w, grid_h = grid.size\n",
        "\n",
        "    for i, img in enumerate(imgs):\n",
        "        grid.paste(img, box=(i % cols * w, i // cols * h))\n",
        "    return grid\n",
        "\n",
        "display(image_grid([init_image, images[0]], 1, 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "760d15e4-edc8-49e7-95fb-94c523723c22",
      "metadata": {
        "id": "760d15e4-edc8-49e7-95fb-94c523723c22"
      },
      "outputs": [],
      "source": [
        "\"\"\"LCM\n",
        "\"\"\"\n",
        "import torch\n",
        "from IPython.display import display\n",
        "from diffusers import StableDiffusionPipeline, LCMScheduler\n",
        "\n",
        "model_id = \"/content/models/stable-diffusion-v1-5\"\n",
        "adapter_id = \"/content/models/lcm-lora-sdv1-5\"\n",
        "\n",
        "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
        "pipe.scheduler = LCMScheduler.from_config(pipe.scheduler.config)\n",
        "pipe = pipe.to(\"cuda\")\n",
        "\n",
        "# 加载lcm lora\n",
        "pipe.load_lora_weights(adapter_id)\n",
        "pipe.fuse_lora()\n",
        "\n",
        "prompt = [\"(taylor swift), 1girl, best quality, extremely detailed\"]\n",
        "negative_prompt=[\"monochrome, lowres, bad anatomy, worst quality, low quality\"]\n",
        "\n",
        "images = pipe(\n",
        "    prompt, negative_prompt=negative_prompt,\n",
        "    num_inference_steps=4, guidance_scale=0\n",
        ").images\n",
        "\n",
        "\n",
        "display(images[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e221465-ba98-4693-be57-a457f009c043",
      "metadata": {
        "id": "7e221465-ba98-4693-be57-a457f009c043"
      },
      "outputs": [],
      "source": [
        "\"\"\"ControlNet-Canny\n",
        "\"\"\"\n",
        "from io import BytesIO\n",
        "\n",
        "import cv2\n",
        "import torch\n",
        "import requests\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "from diffusers import StableDiffusionControlNetPipeline, UniPCMultistepScheduler, ControlNetModel\n",
        "\n",
        "model_id = \"/content/models/stable-diffusion-v1-5\"\n",
        "control_net_path = \"/content/models/ControlNet-v1-1/control_v11p_sd15_canny.pth\"\n",
        "controlnet = ControlNetModel.from_single_file(control_net_path, torch_dtype=torch.float16)\n",
        "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
        "    model_id, controlnet=controlnet, torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
        "pipe = pipe.to(\"cuda\")\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/lllyasviel/ControlNet/main/test_imgs/man.png\"\n",
        "response = requests.get(url)\n",
        "init_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "# init_image = init_image.resize((512, 512))\n",
        "\n",
        "image = np.array(init_image)\n",
        "low_threshold = 100\n",
        "high_threshold = 200\n",
        "image = cv2.Canny(image, low_threshold, high_threshold)\n",
        "image = image[:, :, None]\n",
        "image = np.concatenate([image, image, image], axis=2)\n",
        "canny_image = Image.fromarray(image)\n",
        "\n",
        "prompt = [\"(taylor swift), 1girl, best quality, extremely detailed\"]\n",
        "negative_prompt=[\"monochrome, lowres, bad anatomy, worst quality, low quality\"]\n",
        "\n",
        "images = pipe(\n",
        "    prompt, canny_image, negative_prompt=negative_prompt,\n",
        "    num_inference_steps=20, controlnet_conditioning_scale=0.5\n",
        ").images\n",
        "\n",
        "\n",
        "def image_grid(imgs, rows, cols):\n",
        "    assert len(imgs) == rows * cols\n",
        "\n",
        "    w, h = imgs[0].size\n",
        "    grid = Image.new(\"RGB\", size=(cols * w, rows * h))\n",
        "    grid_w, grid_h = grid.size\n",
        "\n",
        "    for i, img in enumerate(imgs):\n",
        "        grid.paste(img, box=(i % cols * w, i // cols * h))\n",
        "    return grid\n",
        "\n",
        "display(image_grid([init_image, canny_image, images[0]], 1, 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d370940-5f3f-406f-b6d8-181537f4f706",
      "metadata": {
        "id": "3d370940-5f3f-406f-b6d8-181537f4f706"
      },
      "outputs": [],
      "source": [
        "\"\"\"ControlNet-Openpose\n",
        "\"\"\"\n",
        "from io import BytesIO\n",
        "\n",
        "import cv2\n",
        "import torch\n",
        "import requests\n",
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "from controlnet_aux import OpenposeDetector\n",
        "from diffusers import StableDiffusionControlNetPipeline, UniPCMultistepScheduler, ControlNetModel\n",
        "\n",
        "model_id = \"/content/models/stable-diffusion-v1-5\"\n",
        "control_net_path = \"/content/models/ControlNet-v1-1/control_v11p_sd15_openpose.pth\"\n",
        "controlnet = ControlNetModel.from_single_file(control_net_path, torch_dtype=torch.float16)\n",
        "pipe = StableDiffusionControlNetPipeline.from_pretrained(\n",
        "    model_id, controlnet=controlnet, torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n",
        "pipe = pipe.to(\"cuda\")\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/lllyasviel/ControlNet/main/test_imgs/pose1.png\"\n",
        "response = requests.get(url)\n",
        "init_image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "# init_image = init_image.resize((512, 512))\n",
        "\n",
        "openpose = OpenposeDetector.from_pretrained(\"lllyasviel/ControlNet\")\n",
        "openpose_image = openpose(init_image, hand_and_face=True)\n",
        "\n",
        "prompt = [\"(taylor swift), 1girl, best quality, extremely detailed\"]\n",
        "negative_prompt=[\"monochrome, lowres, bad anatomy, worst quality, low quality\"]\n",
        "\n",
        "images = pipe(\n",
        "    prompt, openpose_image, negative_prompt=negative_prompt,\n",
        "    num_inference_steps=20, controlnet_conditioning_scale=0.5\n",
        ").images\n",
        "\n",
        "\n",
        "def image_grid(imgs, rows, cols):\n",
        "    assert len(imgs) == rows * cols\n",
        "\n",
        "    w, h = imgs[0].size\n",
        "    grid = Image.new(\"RGB\", size=(cols * w, rows * h))\n",
        "    grid_w, grid_h = grid.size\n",
        "\n",
        "    for i, img in enumerate(imgs):\n",
        "        grid.paste(img, box=(i % cols * w, i // cols * h))\n",
        "    return grid\n",
        "\n",
        "display(image_grid([init_image, openpose_image, images[0]], 1, 3))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}